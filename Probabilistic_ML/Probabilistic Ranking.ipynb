{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import scipy as s\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ranking systems have many different applications, however most common ones are poor as they rely on arbitrary conventions which lead to poor proformance. In general, they try to answer they question **What is the probability that player 1 defeats player 2?**. In order to determine this probabilistically, there are a number of things to consider:\n",
    "\n",
    "- Considers who you played against.\n",
    "- Must be robust against players who have not played against eachother.\n",
    "- Give a good estimate at any point in the season.\n",
    "- Take into account performance inconsistancy.\n",
    "\n",
    "Therefore, what we want to infer is the player's **skill**, $w$. These skills must be comparable (i.e. a player with a higher skill must be more likely to win), and as such we want to do a probabilstic inference of a player's skill and be able to compute the probability of a game's outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A generative model for skill\n",
    "\n",
    "A summary of a generative model for game outcomes can be defined as:\n",
    "\n",
    "1. **Skills** Take two players with known skills, $$w_i \\in \\mathbb{R}$$\n",
    "2. **Skill Difference**: $$s = w_1 - w_2$$\n",
    "3. **Performance Difference**: Add noise ($n \\sim \\mathcal{N}(0, 1))$ to account for performance inconsistance: $$t = s + n$$\n",
    "4. **Game outcome** is given by $y=sign(t)$:\n",
    "    - $y = +1$ means player 1 wins\n",
    "    - $y = -1$ means player 2 wins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![FactorGraph](Figures/FactorGraph.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The probability of a player winning given their skills\n",
    "    \n",
    "Therefore we can work out the probability that player 1 wins given the players skills':\n",
    "\n",
    "$$p(y| w_1, w_2) = \\int \\int p(y|t) p(t|s) p(s | w_1, w_2) dsdt$$\n",
    "$$ = \\int p(y|t) p(t|w_1, w_2) dt$$\n",
    "$$ = \\int^{+\\infty}_{-\\infty} \\delta(y - sign(t)) \\mathcal{N}(t; w_1 - w_2, 1) dt$$\n",
    "$$ = \\int^{+\\infty}_{-\\infty} \\delta(1 - sign(yt)) \\mathcal{N}(yt; y(w_1 - w_2), 1) dt $$\n",
    "set z = yt and note the change in limits and dt:\n",
    "$$ = \\int^{+y\\infty}_{-y\\infty} \\delta(1 - sign(z)) \\mathcal{N}(z; y(w_1 - w_2), 1) y dz $$\n",
    "$$ = \\int^{+\\infty}_{-\\infty} \\delta(1 -sign(z)) \\mathcal{N}(z; y(w_1 - w_2), 1) dz $$\n",
    "And rearrange the limits:\n",
    "$$ = \\int^{+\\infty}_{0} \\mathcal{N}(z; y(w_1 - w_2), 1) dz $$\n",
    "using $x = y(w_1-w_2) - z$\n",
    "$$ = \\int^{y(w_1 - w_2)}_{-\\infty} \\mathcal{N}(x; 0, 1) dx $$\n",
    "$$ = \\Phi(y(w_1 - w_2))$$\n",
    "\n",
    "where $\\Phi(a)$ is the gaussian c.d.f, or the *probit* function.\n",
    "\n",
    "For the probability of player 1 winning, we simply use $p(y=1| w_1, w_2) = p(t>0| w_1, w_2) = \\Phi(w_1 - w_2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PerformanceDifference](Figures/PerformanceDifference.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the skills (the posterior)\n",
    "\n",
    "$$p(w_1, w_2 | y) = \\frac{Priors \\times Likelihood}{Evidence}$$\n",
    "<br>\n",
    "$$ = \\frac{p(w_1)p(w_2) \\times p(y|w_1, w_2)}{\\int \\int p(w_1)p(w_2) \\times p(y|w_1, w_2) dw_1 dw_2}$$\n",
    "<br>\n",
    "$$ = \\frac{\\mathcal{N}(w_1; \\mu_1, \\sigma_1^2) \\mathcal{N}(w_2; \\mu_2, \\sigma_2^2) \\times \\Phi(y(w_1 - w_2))}{\\int \\int \\mathcal{N}(w_1; \\mu_1, \\sigma_1^2) \\mathcal{N}(w_2; \\mu_2, \\sigma_2^2) \\times \\Phi(y(w_1 - w_2)) dw_1 dw_2}$$\n",
    "\n",
    "The joint posterior over skills does not have a closed form as the probit function is not closed. Additionally, $w_1$ and $w_2$ have become correlated due to the priors and therefore does not factorise, nor is it a gaussian density function.\n",
    "\n",
    "Fortunately, the evidence does have a closed form:\n",
    "\n",
    "$$p(y) = \\int \\int \\mathcal{N}(w_1; \\mu_1, \\sigma_1^2) \\mathcal{N}(w_2; \\mu_2, \\sigma_2^2) \\times \\Phi(y(w_1 - w_2)) dw_1 dw_2 = \\Phi \\bigg(\\frac{y(\\mu_1 - \\mu_2)}{\\sqrt{1 + \\sigma_1^2 + \\sigma_2^2}} \\bigg)$$\n",
    "\n",
    "This is effectively a smoother version of the likelihood as we are using mean skills of each player, and normalising over their variances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Conditionals\n",
    "\n",
    "For notation we have have $g = \\{1, ..., G\\}$ games where $I_g$ is the id of player 1, and $J_g$ is the id of player 1. The outcome of game g is:\n",
    "\n",
    "$$y_g = \\bigg{\\{} \\begin{array}{c} +1 \\; if \\; I_g \\; wins \\\\ -1 \\; if \\; J_g \\; wins \\end{array}$$\n",
    "\n",
    "Note also that the vector of skills is $\\boldsymbol{w} = [w_1, ..., w_d]^T$, (d=107 players) and the vector of performance differences is $\\boldsymbol{t} = [t_1, ..., t_G]^T$.\n",
    "\n",
    "In **True Skill** the joint distribution $p(w_{1:d}, t_{1:G})$ is intractable. However, the two conditionals are tractable and easily derived to be:\n",
    "\n",
    "- **Performance given skill difference**: $p(t_g | w_{I_g}, w_{J_g}, y_g) \\propto \\delta(y_g - sign(t_g)) \\mathcal{N}(t_g;  w_{I_g} - w_{J_g}, 1)$\n",
    "- **Skills given performances**: $p( \\boldsymbol{w} | \\boldsymbol{t}, \\boldsymbol{y}) = p( \\boldsymbol{w} | \\boldsymbol{t}) \\propto p(\\boldsymbol {w}; \\boldsymbol{\\mu}_0, \\Sigma_0) \\prod_{g=1}^G \\mathcal{N}(\\boldsymbol{w}; \\boldsymbol{\\mu}_g, \\Sigma_g)$\n",
    "\n",
    "In order to calculate these, we can take the performance difference directly from the derivation of the evidence:\n",
    "\n",
    "$$p(t | w_1, w_2, y) = p(y|t) p(t|w_1, w_2) = \\delta(y - sign(t)) \\mathcal{N}(t; w_1 - w_2, 1) dt$$\n",
    "\n",
    "and similarly, we note that the priors for skill are $p(w_i) = \\mathcal{N}(w_i | u_i, \\sigma_i^2) \\implies p(\\boldsymbol{w}) = \\prod_{i=1}^d p(w_i) = \\mathcal{N}(\\boldsymbol{w} ; \\boldsymbol{\\mu}_0, \\Sigma_0)$. Thus,\n",
    "\n",
    "$$p( \\boldsymbol{w} | \\boldsymbol{t}) = \\frac{p(\\boldsymbol{w})p(\\boldsymbol{t}| \\boldsymbol{w})}{p(\\boldsymbol{t})} \\propto p(\\boldsymbol{w}) \\prod_{g=1}^G p(t_g | w_{I_g}, w_{J_g}) = \\mathcal{N}(\\boldsymbol{w} ; \\boldsymbol{\\mu}_0, \\Sigma_0) \\prod_{g=1}^G \\mathcal{N}(\\boldsymbol{w}; \\boldsymbol{\\mu}_g, \\Sigma_g)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "mat = scipy.io.loadmat('tennis_data.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rafael-Nadal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Juan-Monaco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Juan-Martin-Del-Potro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mardy-Fish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Roger-Federer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name\n",
       "0           Rafael-Nadal\n",
       "1            Juan-Monaco\n",
       "2  Juan-Martin-Del-Potro\n",
       "3             Mardy-Fish\n",
       "4          Roger-Federer"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>winner</th>\n",
       "      <th>loser</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   winner  loser\n",
       "0       1      2\n",
       "1       1      3\n",
       "2       1      3\n",
       "3       1      3\n",
       "4       1      3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1801"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PLAYERS = pd.DataFrame(mat['W'], columns=['name']).applymap(lambda x: x[0])  # remove the list around each players name\n",
    "GAMES = pd.DataFrame(mat['G'], columns=['winner', 'loser'])\n",
    "display(PLAYERS.head(), GAMES.head())\n",
    "display(len(PLAYERS), len(GAMES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gibbs Rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integration of an Intractable Posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basis of Monte-Carlo approximation is:\n",
    "$$\\mathbb{E}_{p(x)} \\big[ \\Phi(x) \\big] \\approx \\hat{\\Phi} = \\frac{1}{T} \\sum^t_{\\tau = 1} \\Phi(x^{(\\tau)}), \\text{  where } x^{(\\tau)} \\sim p(x)$$\n",
    "\n",
    "Note that $x^{(\\tau)}$ is the $\\tau$th d-dimensional sample from the distribution p(x), which is analytically intractable (and typically d>>1).\n",
    "\n",
    "This is infact an unbiased estimate, with $Var[\\hat{\\Phi}] = \\frac{Var[\\Phi]}{T}$. Note $Var[\\Phi] = \\int (\\Phi(x) - \\mathbb{E}[\\Phi])^2 p(x) dx$. Note that this is independent of dimension d, of x.\n",
    "\n",
    "\n",
    "How do we generate samples from p(x)? If the distribution has a standard form then we could generate independent samples, however, it is often difficult to sample from this joint distribution if it is within a high dimensional space (the curse of dimensionality). In order to get around this we can use **Gibbs Sampling**, which uses a Markov Chain to generate dependent samples from the desired distribution:\n",
    "\n",
    "$$x_i' \\sim  p(x_i | x_{1, 2, ..., i-1, i+1, ..., d})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gibbs Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gibbs sampling assumes that we know the forms of the joint and conditional distributions. Given that $p(x, y) \\propto p(x|y)$, we can thus sample from the conditional distributions. E.g. A number of experiments, k, are performed where we flip a coin n times (unknown). The coin is biased such that $p(heads) = \\theta$.\n",
    "\n",
    "- $n \\sim U(5, 8)$\n",
    "- $\\theta \\sim U(0, 1) = beta(1, 1)$\n",
    "\n",
    "This gives us a sequence with the number of heads that were thrown: $X = (x_1, x_2, .., x_k)$. We are aiming to find $p(n, \\theta | X) \\propto p(X | n, \\theta)p(n, \\theta) = Zp(X | n, \\theta)$. \n",
    "\n",
    "$$p(n, \\theta | X) \\propto \\prod^k_{i=1} C^n_{x_i} \\theta^{x_i} (1-\\theta)^{n-x_i}$$\n",
    "$$= \\theta^{\\sum+_{i=1}^k x_i}(1-\\theta)^{\\sum_{i=1}^k n-x_i} \\prod_{i=1}^k  C^n_{x_i}$$\n",
    "$$\\theta^k\\hat{x}(1-\\theta)^{k(n-\\hat{x})} \\prod_{i=1}^k  C^n_{x_i}$$\n",
    "\n",
    "Above is the joint probability upto a constant of proportionality. Next we need to find the conditionals, which we can do by simply splitting the above into $f_1(n)f_2(\\theta)$.\n",
    "\n",
    "$$p(\\theta |n, X) \\propto \\theta^k\\hat{x}(1-\\theta)^{k(n-\\hat{x})} = beta(k\\hat{x}+1, k(n-\\hat{x})+1)$$\n",
    "$$p(n |\\theta, X) \\propto (1-\\theta)^{kn}\\prod_{i=1}^k  C^n_{x_i}$$\n",
    "\n",
    "Note that there is no \"nice\" form for $p(n | \\theta, X)$ here. But what we can do, is we know $n \\in \\{5, 6, 7, 8\\}$, so we can use these values and sum them up to get a categorical distribution by normalising with them.\n",
    "\n",
    "Gibbs sampling here then works simply by choosing some random starting point and then iteratively sampling from each of these two conditionals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For **our problem**, we have 107 different players, and hence are sampling from 107 different conditional distributions for thier skills. Consider the distribution $p(x) = p(x_{1:d})$. The gibbs sampling algorithm is as follows:\n",
    "\n",
    "\n",
    "> 1. Initialise $\\{x_i : i = 1, .., d\\}$\n",
    "> 2. For $\\tau = 1, ..., T$: <br>\n",
    "    > &nbsp;&nbsp;&nbsp;&nbsp; For $i = 1, ..., d$: <br>\n",
    "        > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Sample $x_{i}^{\\tau+1} \\sim p(x_i|x_{\\backslash i}^{(\\tau)})$\n",
    "        \n",
    "**Try over-relaxation?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![GibbsSampling](Figures/GibbsSampling.jpg)\n",
    "\n",
    "Above is an illustration of Gibbs sampmling by alternate updates of two variables, whose distribution is a correlated GAussian. The step size is govened by the standard deviation of the  <span style=\"color:green\">condiditional distribution</span>, and is O(l), leading to slow progression in the direction of elongation of the <span style=\"color:red\">joint distribution</span>. The number of steps needed to obtain an independent sample from the distribution is $O((\\frac{L}{l})^2)$. I.e. **Strong correlations slow down gibbs sampling**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **True Skill** the joint distribution $p(w_{1:d}, t_{1:G})$ is intractable. However, the two conditionals are tractable and have already been derived to be:\n",
    "\n",
    "- **Performance given skill difference**: $p(t_g | w_{I_g}, w_{J_g}, y_g) = \\frac{p(y|t) p(t|w_1, w_2)}{p(y)} \\propto \\delta(y_g - sign(t_g)) \\mathcal{N}(t_g;  w_{I_g} - w_{J_g}, 1)$\n",
    "- **Skills given performances**: $p( \\boldsymbol{w} | \\boldsymbol{t}, \\boldsymbol{y}) = p( \\boldsymbol{w} | \\boldsymbol{t}) \\propto p(\\boldsymbol{w}) \\prod_{g=1}^G p(t_g | w_{I_g}, w_{J_g})  =  p(\\boldsymbol {w}; \\boldsymbol{\\mu}_0, \\Sigma_0) \\prod_{g=1}^G \\mathcal{N}(\\boldsymbol{w}; \\boldsymbol{\\mu}_g, \\Sigma_g)$\n",
    "\n",
    "We can describe the two above distributions in more detail:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the conditional posterior of skills (skills given performances) we get:\n",
    "\n",
    "$$p(t_g|w_{I_g}, w_{J_g}) = \\mathcal{N}(t_g; (w_{I_g} - w_{J_g}), 1) \\propto exp(-\\frac{1}{2}(t_g-(w_{I_g} - w_{J_g}))^2 )$$\n",
    "\n",
    "renaming $t_g = \\mu_1 - \\mu_2$\n",
    "\n",
    "$$\\propto exp(-\\frac{1}{2}((w_{I_g} - \\mu_1) - ( w_{J_g} - \\mu_2) )^2 )$$\n",
    "\n",
    "$$ \\propto exp \\bigg( -\\frac{1}{2} \n",
    "\\left[\\begin{array}{c} w_{I_g} - \\mu_1 \\\\ w_{J_g} - \\mu_2 \\end{array}\\right]\n",
    "\\left[\\begin{array}{cc} 1 & -1 \\\\ -1 & 1 \\end{array}\\right]\n",
    "\\left[\\begin{array}{c} w_{I_g} - \\mu_1 \\\\ w_{J_g} - \\mu_2 \\end{array}\\right] \\bigg)\n",
    "$$\n",
    "\n",
    "Recall that when multiplying gaussians, the precisions add up and the means weighted by the precisions also add up:\n",
    "\n",
    "- $\\Sigma_c^{-1} = \\Sigma_a^{-1} + \\Sigma_b^{-1}$\n",
    "- $\\mu_c = \\Sigma_c(\\Sigma_a^{-1}\\mu_a +  \\Sigma_b^{-1} \\mu_b)$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$p(\\boldsymbol {w}; \\boldsymbol{\\mu}_0, \\Sigma_0) \\prod_{g=1}^G \\mathcal{N}(\\boldsymbol{w}; \\boldsymbol{\\mu}_g, \\Sigma_g) = \\mathcal{N}(\\boldsymbol{w}; \\mu, \\Sigma)$$\n",
    "\n",
    "Where each game precision, $\\Sigma_g^{-1}$ contains only 4 non-zero entries, the full precisions and means are:\n",
    "\n",
    "- $\\Sigma^{-1} = \\Sigma_0^{-1} + \\sum_{g=1}^G \\Sigma_g^{-1}  = \\Sigma_0^{-1} + \\tilde{\\Sigma}^{-1}$\n",
    "- $\\mu = \\Sigma(\\Sigma_0^{-1}\\mu_0 + \\sum_{g=1}^G \\Sigma_g^{-1}\\mu_g) = \\Sigma(\\Sigma_0^{-1}\\mu_0 + \\tilde{\\mu})$\n",
    "\n",
    "defining these further:\n",
    "\n",
    "$$\\tilde{\\mu}_i = \\sum_{g=1}^G t_g(\\delta(i-I_g) - \\delta(i-J_g))$$\n",
    "\n",
    "\n",
    "$$ [ \\tilde{\\Sigma}^{-1} ]_{ij}  = \\bigg\\{  \\begin{array}{c} \\sum_{g=1}^G \\delta(i-I_g) + \\delta(i-J_g) \\; \\; \\; for \\; i = j \\\\ \\sum_{g=1}^G \\delta(i-I_g)\\delta(j-J_g) + \\delta(i-J_g)\\delta(j-I_g) \\; \\; \\; for \\; i \\neq j  \\end{array} $$\n",
    "\n",
    "The conditional skills can be jointly sampled directly using the cholesky factorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the conditional posterior of the performances (performance given skill difference), \n",
    "\n",
    "we get a truncated gaussian that needs no further analysis, however the question becomes how do we sample from it? The inverse transformation method, or rejection sampling are both possibilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo. \n",
    "1. Read the gibbs section of Pattern Recog and add anything from there into here\n",
    "1. Reread the notes and add the specific maths\n",
    "1. implement the gibbs sampling algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_distribution(x, mu, s):\n",
    "    exponent = -0.5*((x-mu)/s)**2\n",
    "    Z = (s * np.sqrt(2*np.pi))\n",
    "    return np.exp(exponent) / Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs_sampler(total_iters, players=PLAYERS, games=GAMES):\n",
    "     skills_container = np.zeros(len(), total_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
